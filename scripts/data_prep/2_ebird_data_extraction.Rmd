---
title: "Extract required observation and sampling data from eBird"
output: html_notebook
bibliography: ../ref.bib  
---


```{r}
source('../env.R')
```
# Objectives
Two objectives in this script:
1. Find all the checklists meeting the @Callaghan_2017 criteria with in our initial city selection polygons
2. Find all the Columbidae species that occur on those checklists.

# Requirements
Install sqllite locally.
Download full eBird data and sampling data.

# eBird Sampling Data

## Check eBird raw sampling data
@eBird2023 data can be downloaded here:
https://science.ebird.org/en/use-ebird-data/download-ebird-data-products

```{r}
EBIRD_SAMPLE_DATA_RAW = '/Users/james/Dropbox/PhD/eBird/ebd_sampling_relNov-2023/ebd_sampling_relNov-2023.txt'
Sys.setenv(EBIRD_SAMPLE_FILE = EBIRD_SAMPLE_DATA_RAW)
```

Read first 5 columns of raw ebird sampling data
```{bash}
head -6 ${EBIRD_SAMPLE_FILE}
```

# Find all eBird localities with valid checklists within our cities
Our ultimate aim is to find all checklists within our cities that match the filters defined by @Callaghan_2017, and store filtered data in cache file.

Our process for doing this is:

1. Using eBird sampling data; find all complete checklists; that are travelling, area, or stationary; and created between the years 2013 to 2023; store these as our first pass.

2. Extract all eBird unique localities from the first pass of checklists

3. Join those localities to our initial city selection vector, and keep any location that is within an urban polygon. This creates a mapping of eBird locality to city.

4. Store both the first pass of checklists and the locality to city mapping in sqlite.

5. Extract all checklists from sqlite that are recorded at an urban locality applying the filters defined by @Callaghan_2017

## Extract first pass to find valid checklists
Read header of eBird sampling data file, and print with column index.
```{bash}
let INDEX=1
head -1 ${EBIRD_SAMPLE_FILE} | sed 's/\t/\n/g' | while read column_header; do 
    echo ${INDEX}: $column_header
    let INDEX=${INDEX}+1
done
```

First we will find all complete checklists between 2013 and 2023, that are one of the reqiured protocols (travelling, stationary, or area).
We will store this first pass at filtering checklists in a temp file:
```{r}
EBIRD_CHECKLIST_FIRST_PASS = filename(TMP_DIR, 'ebird_checklist_first_pass_cache.txt')
Sys.setenv(EBIRD_CHECKLIST_FIRST_PASS_FILE = EBIRD_CHECKLIST_FIRST_PASS)
```

```{bash}
echo ${EBIRD_CHECKLIST_FIRST_PASS_FILE}
```

Here we cut for the following eBird raw columns

* 13: LOCALITY ID
* 15: LATITUDE
* 16: LONGITUDE
* 17: OBSERVATION DATE
* 20: SAMPLING EVENT IDENTIFIER
* 21: PROTOCOL TYPE
* 24: DURATION MINUTES
* 25: EFFORT DISTANCE KM
* 28: ALL SPECIES REPORTED
* 29: GROUP IDENTIFIER

and then filter such that

* "ALL SPECIES REPORTED" is true ("1")
* "PROTOCOL TYPE" IN (Stationary, Traveling, Area)
* "OBSERVATION DATE" >= 2013
  ((201[3-9])|(202[0-3]))-[0-1][0-9]-[0-9]{2}

to collect a unique set of (LOCALITY ID, LATITUDE, LONGITUDE)

We use bash for streaming the file and then

1. `cut` the columns we want to use

2. `grep` to find all complete checklists (line ending with 1)

3. `grep` to check for required protocols

4. `grep` to check for years 2013 -> 2023

```{bash}
COLUMNS=13,15,16,17,20,21,24,25,28,29
echo -e "locality_id\tlat\tlong\tobs_date\tchecklist_id\tprotocol\tduration\teffort_km\tcomplete\tgroup_id" > ${EBIRD_CHECKLIST_FIRST_PASS_FILE} 
cat ${EBIRD_SAMPLE_FILE} | cut -f${COLUMNS} | grep -E "^L[0-9]+\t[-0-9.]+\t[-0-9.]+\t((201[3-9])|(202[0-3]))-[0-1][0-9]-[0-9]{2}\tS[0-9]+\t(Traveling|Area|Stationary)\t[0-9]*\t[0-9]*\t1" >> ${EBIRD_CHECKLIST_FIRST_PASS_FILE}
```

## Get all localities from valid checklists
Now we can get a unique list of localities from our first pass.
```{r}
EBIRD_ALL_LOCALITIES = filename(TMP_DIR, 'ebird_all_localities.txt')
Sys.setenv(EBIRD_ALL_LOCALITIES_FILE = EBIRD_ALL_LOCALITIES)
```

```{bash}
cat ${EBIRD_CHECKLIST_FIRST_PASS_FILE} | cut -f1,2,3 |  awk '!a[$0]++' >> ${EBIRD_ALL_LOCALITIES_FILE}
```

```{r}
all_ebird_locations = read_tsv(EBIRD_ALL_LOCALITIES)
head(all_ebird_locations)
```

## Find all urban localities using initial city selection
Read in our initial city vectors, we will join these to our checklists
```{r}
initial_city_selection = read_sf(filename(mkdir(GEO_WORKING_OUTPUT_DIR, 'cities'), 'initial_selection.shp'))
```

Turn our ebird locations into actual points
```{r}
all_ebird_locations_sf <- st_as_sf(all_ebird_locations, coords = c("long", "lat"))
st_crs(all_ebird_locations_sf) <- st_crs(initial_city_selection)

write_sf(all_ebird_locations_sf, filename(mkdir(GEO_WORKING_OUTPUT_DIR, 'ebird'), 'all_locations.shp'))
```

Join the eBird locations onto the city vectors, giving each location a city and discarding all other locations
```{r}
sf::sf_use_s2(FALSE)

localities_in_cities <- 
  st_join(all_ebird_locations_sf, initial_city_selection) %>%  # spatial join to get intersection of points and poly
  filter(!is.na(city_id))

head(localities_in_cities)
```

```{r}
localities_in_cities[localities_in_cities$locality_id == 'L6081908',] # Manchester
localities_in_cities[localities_in_cities$locality_id == 'L13024221',] # Bogota
```

```{r}
nrow(localities_in_cities)
```

Store urban locality to city mapping IDs in file.
```{r}
EBIRD_LOCALITY_TO_CITY = filename(EBIRD_WORKING_OUTPUT_DIR, 'locality_to_city.txt')
write_csv(data.frame(
  locality_id = localities_in_cities$locality_id, 
  city_id = localities_in_cities$city_id,
  city_name= localities_in_cities$city_name
  ), EBIRD_LOCALITY_TO_CITY)
```

Clean up by removing two large data frames of all ebird localities
```{r}
rm(all_ebird_locations)
rm(all_ebird_locations_sf)
```

## Store data into SQLLite
```{r}
Sys.setenv(EBIRD_LOCALITY_TO_CITY_FILE = EBIRD_LOCALITY_TO_CITY)
```

```{r}
CHECKLIST_DATABASE_FILE = filename(TMP_DIR, 'checklists.sqlite.db')
Sys.setenv(CHECKLIST_DB = CHECKLIST_DATABASE_FILE)
```

```{bash}
sqlite3 ${CHECKLIST_DB} "CREATE TABLE checklists (locality_id NVARCHAR(12), lat FLOAT, long FLOAT, obs_date DATE, checklist_id NVARCHAR(12), protocol NVARCHAR(12), duration INTEGER, effort_km INTEGER, complete TINYINT, group_id NVARCHAR(12));"
```

```{bash}
echo -e ".separator \\t\n.import ${EBIRD_CHECKLIST_FIRST_PASS_FILE} checklists" > /tmp/import_command.sqlite
sqlite3 ${CHECKLIST_DB} < /tmp/import_command.sqlite
```

```{bash}
sqlite3 ${CHECKLIST_DB} "CREATE TABLE locality_to_city (locality_id NVARCHAR(12), city_id INT, city_name NVARCHAR(100));"
```

```{bash}
sqlite3 ${CHECKLIST_DB} ".import --csv ${EBIRD_LOCALITY_TO_CITY_FILE} locality_to_city"
```

## Export urban checklists from sqlite
Finally we can filter out first pass of checklists to only urban checklists, and checklists satisfy all of the criteria of @Callaghan_2017.

First we need to deduplciate by group_id to ensure we only have one checklist per group:
```{bash}
sqlite3 ${CHECKLIST_DB} "CREATE TABLE unique_checklists AS
WITH dedupe_group AS (
  SELECT 
    checklist_id,
    locality_id,
    duration,
    obs_date,
    effort_km,
    lat,
    long,
    ROW_NUMBER() OVER (PARTITION BY group_id ORDER BY checklist_id) row_number
  FROM checklists
  WHERE group_id != ''
), non_group AS (
  SELECT 
    checklist_id,
    locality_id,
    duration,
    obs_date,
    effort_km,
    lat,
    long
  FROM checklists
  WHERE group_id = ''
)
SELECT checklist_id, locality_id, lat, long, duration, effort_km, obs_date 
FROM dedupe_group 
WHERE row_number = 1
UNION ALL
SELECT checklist_id, locality_id, lat, long, duration, effort_km, obs_date 
FROM non_group;"
```

```{r}
EBIRD_URBAN_CHECKLISTS = filename(EBIRD_WORKING_OUTPUT_DIR, "urban_checklists.csv")
Sys.setenv(EBIRD_URBAN_CHECKLISTS_FILE = EBIRD_URBAN_CHECKLISTS)
```

Then we apply filters such that:

1. duration >= 5 AND duration <= 240

2. effort_km = '' OR (effort_km >= 0 AND effort_km <= 10)

```{bash}
echo -e ".mode csv\n.headers on\nSELECT checklist_id, locality_id, city_id, city_name, lat, long, duration, obs_date FROM unique_checklists JOIN locality_to_city USING (locality_id) WHERE duration >= 5 AND duration <= 240 AND (effort_km = '' OR (effort_km >= 0 AND effort_km <= 10))" > /tmp/export_command.sqlite
sqlite3 ${CHECKLIST_DB} < /tmp/export_command.sqlite > ${EBIRD_URBAN_CHECKLISTS_FILE}
```

## Examine number of urban checklists per city
```{r}
urban_checklists = read_csv(EBIRD_URBAN_CHECKLISTS)
head(urban_checklists)
```
Effort per city
```{r}
urban_checklists %>% group_by(city_id, city_name) %>%
  summarise(total_checklists = n(), total_effort = sum(duration)) %>%
  arrange(total_checklists)
```

Cities from initial selection that have no qualifiying checklists
```{r}
initial_city_selection$city_name[!(initial_city_selection$city_id %in% urban_checklists$city_id)]
```

# Find all Columbidae records
Here we want to find all Columbidae records from eBird that are present on our extracted checklists.

The process we follow to complete this is:

1. A first pass extraction from the eBird raw observation data, extracting Columbidae records on complete checklists, recorded between 2013 and 2023.

2. We then import these records, along with the urban checklists into a sqlite database.

3. Extract all Columbidae records that occur on our urban checklists.


## Find the required taxonomy from the eBird Clements taxonomy
See here for links to download:
https://science.ebird.org/en/use-ebird-data/the-ebird-taxonomy

```{r}
EBIRD_TAXONOMY = '/Users/james/Dropbox/PhD/eBird/taxonomy_2023/ebird_taxonomy_v2023.csv'
taxonomy = read_csv(EBIRD_TAXONOMY)
head(taxonomy)
```

Find the Columbiformes
```{r}
columbiformes_taxonomy_all = taxonomy[taxonomy$ORDER == 'Columbiformes', c('TAXON_ORDER', 'CATEGORY', 'SCI_NAME', 'PRIMARY_COM_NAME')]
columbiformes_taxonomy_all
```

Filter further to include only full species, from the non-species list (below) we also want to keep:

```1875	form	    Columba livia (Wild type)```

```{r}
columbiformes_taxonomy_all[!(columbiformes_taxonomy_all$CATEGORY %in% c('species', 'issf', 'domestic')),]
```

```{r}
COLUMBIDAE_TAXONOMY = filename(EBIRD_WORKING_OUTPUT_DIR, 'taxonomy.csv')
columbiformes_taxonomy = columbiformes_taxonomy_all[columbiformes_taxonomy_all$CATEGORY %in% c('species', 'issf', 'domestic') | columbiformes_taxonomy_all$TAXON_ORDER == 1875 & !is.na(columbiformes_taxonomy_all$TAXON_ORDER),]
write_csv(columbiformes_taxonomy, COLUMBIDAE_TAXONOMY)
columbiformes_taxonomy
```

We can construct an initial filter for species by checking the TAXON_ORDER is between 1874 and 2855. We can use this to filter for observations from the eBird raw observations file.
```{r}
taxonomy[taxonomy$TAXON_ORDER >= 1874 & taxonomy$TAXON_ORDER <= 2855,c('ORDER', 'SCI_NAME', 'PRIMARY_COM_NAME')]
```

## Check eBird raw data
@eBird2023 data can be downloaded here:
https://science.ebird.org/en/use-ebird-data/download-ebird-data-products

```{r}
EBIRD_DATA_RAW = '/Users/james/Dropbox/PhD/eBird/ebd_relNov-2023/ebd_relNov-2023.txt'
Sys.setenv(EBIRD_FILE = EBIRD_DATA_RAW)
```

Read first 5 columns of raw ebird sampling data
```{bash}
head -6 ${EBIRD_FILE}
```

## Extract a first pass of required observations
Here we get all pigeon records on complete checklists of the required protocol within the years 2013 to 2023

Read header of eBird sampling data file, and print with column index.
```{bash}
let INDEX=1
head -1 ${EBIRD_FILE} | sed 's/\t/\n/g' | while read column_header; do 
    echo ${INDEX}: $column_header
    let INDEX=${INDEX}+1
done
```

```{r}
EBIRD_OBSERVATIONS_FIRST_PASS = filename(TMP_DIR, 'ebird_obs_first_pass_cache.txt')
Sys.setenv(EBIRD_OBSERVATIONS_FIRST_PASS_FILE = EBIRD_OBSERVATIONS_FIRST_PASS)
```

* 3:  TAXONOMIC ORDER
* 11: OBSERVATION COUNT
* 31: OBSERVATION DATE
* 34: SAMPLING EVENT IDENTIFIER
* 35: PROTOCOL TYPE
* 41: NUMBER OBSERVERS
* 42: ALL SPECIES REPORTED
* 43: GROUP IDENTIFIER

Here we use bash and grep to create an initial pass of checklists by filtering for:

1. TAXONOMIC ORDER is between 1874 and 2855 
((187[4-9])|(18[5-9][0-9])|(19[0-9]{2})|(2[0-7][0-9][0-9])|(28[0-4][0-9])|(285[0-5]))

2. OBSERVATION DATE is between 2013 and 2023
((201[3-9])|(202[0-3]))-[0-1][0-9]-[0-9]{2}

3. PROTOCOL TYPE is one of Traveling|Area|Stationary
(Traveling|Area|Stationary)

4. ALL SPECIES REPORTED is true

```{bash}
COLUMNS=3,11,31,34,35,41,42,43
echo -e "taxon_id\tobs_count\tobs_date\tchecklist_id\tprotocol\tnumber_observers\tis_complete\tgroup_id" > ${EBIRD_OBSERVATIONS_FIRST_PASS_FILE} 
cat ${EBIRD_FILE} | cut -f${COLUMNS} | grep -E "^((187[4-9])|(18[5-9][0-9])|(19[0-9]{2})|(2[0-7][0-9][0-9])|(28[0-4][0-9])|(285[0-5]))\t([0-9]+|X)\t((201[3-9])|(202[0-3]))-[0-1][0-9]-[0-9]{2}\tS[0-9]+\t(Traveling|Area|Stationary)\t[0-9]+\t1\t" >> ${EBIRD_OBSERVATIONS_FIRST_PASS_FILE} 
```

## Import observations into SQLLite
```{r}
OBSERVATION_DATABASE_FILE = filename(TMP_DIR, 'observations.sqllite.db')
Sys.setenv(OBSERVATION_DB = OBSERVATION_DATABASE_FILE)
```

Import Columbidae observations
```{bash}
sqlite3 ${OBSERVATION_DB} "CREATE TABLE observations (taxon_id NVARCHAR(4), obs_count NVARCHAR(4), obs_date DATE, checklist_id NVARCHAR(12), protocol NVARCHAR(12), number_observers INTEGER, is_complete TINYINT, group_id NVARCHAR(12));"
```

```{bash}
echo -e ".separator \\t\n.import ${EBIRD_OBSERVATIONS_FIRST_PASS_FILE} observations" > /tmp/import_command.sqlite
sqlite3 ${OBSERVATION_DB} < /tmp/import_command.sqlite
```

Import urban checklists
```{bash}
sqlite3 ${OBSERVATION_DB} "CREATE TABLE checklists (checklist_id NVARCHAR(12), locality_id NVARCHAR(12), city_id INT, city_name NVARCHAR(100), duration INTEGER, obs_date DATE);"
```

```{bash}
sqlite3 ${OBSERVATION_DB} ".import --csv ${EBIRD_URBAN_CHECKLISTS_FILE} checklists"
```

Import Columbidae taxonomy
```{r}
Sys.setenv(COLUMBIDAE_TAXONOMY_FILE = COLUMBIDAE_TAXONOMY)
```

```{bash}
sqlite3 ${OBSERVATION_DB} "CREATE TABLE taxonomy (taxon_id NVARCHAR(4), category NVARCHAR(10), species_name NVARCHAR(200), common_name NVARCHAR(200));"
```

```{bash}
sqlite3 ${OBSERVATION_DB} ".import --csv ${COLUMBIDAE_TAXONOMY_FILE} taxonomy"
```

## Export observations into SQLLite
First we need to handle the group ID, we only want one checklist per group ID.

```{r}
COLUMBIDAE_OBSERVATIONS = filename(EBIRD_WORKING_OUTPUT_DIR, 'urban_columbidae.csv')
Sys.setenv(COLUMBIDAE_OBSERVATIONS_FILE = COLUMBIDAE_OBSERVATIONS)
```

```{r}
echo -e ".mode csv\n.headers on\nSELECT obs_count AS observation_count,checklist_id,number_observers,city_id,city_name,duration,species_name,common_name FROM observations JOIN checklists USING(checklist_id) JOIN taxonomy USING(taxon_id)" > /tmp/export_command.sqlite
sqlite3 ${OBSERVATION_DB} < /tmp/export_command.sqlite > ${COLUMBIDAE_OBSERVATIONS_FILE}
```

## Examine urban columbidae found
```{r}
urban_pigeons = read_csv(COLUMBIDAE_OBSERVATIONS)
head(urban_pigeons)
```
Species in Manchester, UK
```{r}
urban_pigeons %>% filter(city_name == 'Manchester') %>% 
  group_by(species_name) %>% 
  summarise(checklist_count = n())
```
Species in Nairobi, Kenya
```{r}
urban_pigeons %>% filter(city_name == 'Nairobi') %>% 
  group_by(species_name) %>% 
  summarise(checklist_count = n())
```

Cities with Woodpigeon
```{r}
urban_pigeons %>% filter(species_name == 'Columba palumbus') %>% 
  group_by(city_name) %>% 
  summarise(checklist_count = n()) %>%
  arrange(checklist_count)
```

Urban pool size
```{r}
urban_pigeons %>% 
  group_by(city_name, city_id) %>% 
  summarise(species_count = n_distinct(species_name)) %>%
  arrange(species_count)
```
